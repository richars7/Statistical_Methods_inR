---
title: "Mini Project 6"
author: "Richa Singh"
date: "12/06/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

Consider the prostate cancer dataset available on eLearning as prostate cancer.csv. It consists of data on 97 men with advanced prostate cancer.We would like to understand how PSA level is related to the other predictors in the dataset. Note
that vesinv is a qualitative variable. You can treat gleason as a quantitative variable.Build a reasonably good linear model for these data by taking PSA level as the response variable. Carefully justify all the choices you make in building the model. Be sure to verify the model assumptions. In case a transformation of response is necessary, try the natural log transformation. Use the final model to predict the PSA level for a patient whose quantitative predictors are at the sample
means of the variables and qualitative predictors are at the most frequent category.

```{r include=FALSE}
rm(list = ls())
graphics.off()
```

```{r}
library(png)
img = readPNG("figure.png")
plot.new() 
rasterImage(img,0,0,1,1)
```
```{r}
data <- read.csv("prostate_cancer(1).csv")
```

Only to check the linear relationship between the variables we plot the correlation graph(here vensiv is treated as quantitative variable but further is changed to nominal variable)

```{r}
library(corrplot)
cor.my_data <- cor(data) 
corrplot(cor.my_data, method = "ellipse")
```

Attaching the data so that we can use the variables directly in the linear model
```{r results="hide"}
str(data)
attach(data)
```

When Plotting the psa variable we see that the distribution is skewed to make it better fit our linear model condition we do the log transformation of the psa variable.
```{r}
plot(psa,col="red")
```

Doing the log transformation
```{r}
plot(log(psa),col="red")
```

Plotting the boxplot of the psa variable we see a lot of outliers in the data telling that we need some kind of transformation to fit our linear model 
```{r}
boxplot(psa,col = "pink")
```

```{r}
boxplot(log(psa),col = "pink")
y<- log(psa)
```

```Converting the int vesinv to factor type before giving it to the linear model so that R can recognize that vesinv is a nominal variable```

```{r}
data$vesinv <- as.factor(data$vesinv)
str(data)
```

```Fit the linear model 1:```

Full model : 
Null Hypothesis : None of the predictors are useful for predicting response.
Alternate Hypothesis : Atleast one of the predictors is useful for predicting the response.
```{r}
fit1 <- lm(y ~ cancervol + vesinv + capspen + gleason + weight + age + benpros)
summary(fit1)
```
From the above results we can see that the cancervol , vesinv, gleason, benpros are the significant predictors. Hence, we reject the null hypothesis.

```Fit the linear model 2:```

Reduced Model: 
Null Hypothesis : None of the predictors are useful for predicting response.
Alternate Hypothesis : Atleast one of the predictors is useful for predicting the response.

We can also say that the reduced model is equivalent to the full model i.e the extra predictors of the full model are unnecessary.
```{r}
fit2 <- update(fit1,.~. - capspen - age - weight)
summary(fit2)
```

From the above results we can see that all are the significant predictors. Hence, we reject the null hypothesis.
We can also see that the value of the adjusted R-square also increases validating the better correctness of the linear model.

```Fit the linear model 3:```

Null Hypothesis : None of the predictors are useful for predicting response.
Alternate Hypothesis : Atleast one of the predictors is useful for predicting the response.

Since capspen was coming out to be an important variable in the correlation plot let's add that in our linear model.
```{r}
fit3 <- update(fit2,.~. + capspen)
summary(fit3)
```

From the above results we can see that the cancervol , vesinv, gleason, benpros are the significant predictors. Hence, we reject the null hypothesis. Also, we can see that the adjusted R-squared value decreases telling that capspen is not an optimal predictor for predicting the response variable.

Doing the ANOVA test: 

```{r}
anova(fit2)
```

```{r}
anova(fit1,fit2,fit3)
```

```{r}
anova(fit2,fit3)
```
Since, it's not a good practise to infer any results from 3 model anova comparison we are calculating the AIC score.

```Checking the best model via AIC```

```{r}
fit.full <- fit1 <- lm(y ~ cancervol + vesinv + capspen + gleason + weight + age + benpros)  
for.aic <- step(lm(y ~ 1), direction = "forward", 
                 scope = formula(fit.full), k = 2, trace = 0)               # forward AIC
for.bic <- step(lm(y ~ 1), direction = "forward", 
                 scope = formula(fit.full), k = log(32), trace = 0)         # forward BIC
back.aic <- step(fit.full, direction = "backward", k = 2, trace = 0)        # backward AIC
back.bic <- step(fit.full, direction = "backward", k = log(32), trace = 0)  # backward BIC

(Adjusted_R.square <- data.frame("Method"=c("for.aic", "for.bic", "back.aic", "back.bic"), 
"Adj.r.square"=c(summary(for.aic)$adj.r.square, summary(for.bic)$adj.r.square, 
summary(back.aic)$adj.r.square, summary(back.bic)$adj.r.square)))
summary(for.aic)
```
```Comparing the AIC scores of the three fitted models```

```{r}
l1 <- glm(fit2)
l2 <- glm(fit1)
l3 <- glm(fit3)
print(l1$aic)
print(l2$aic)
print(l3$aic)
```
We can see from the above results l1 or fit2 linear model has the lowest aic score telling that it's the best model among all the models

**From the above validations we can see that the fit2 model is the best model**


```Now we are doing Model evaluation whether the fitted model 2 is the good representation of the linear model```

```{r}
plot(fitted(fit2),resid(fit2),col="red", main = "Residual Plot")
abline(h=0)
```
From the above Residual Plot we see that the mean is zero and not much change in the vertical spread i.e the standard deviation is constant telling that the linear model is a good estimate.

```{r}
qqnorm(resid(fit2),col="red")
qqline(resid(fit2))
```

Initially, we assumed that the Residual error is independent and identically distributed coming from a normal distribution with mean of 0 and s.d of sigma-squared. To validate that assumption, plotting the qqplot of the fitted model we see that the data is almost normally distriuted.

```{r}
plot(resid(fit2),type = "l",main = "Time Series Plot",col="red")
abline(h=0)
```
In the ideal time series plot, there should be no dependence which verifies the independence assumption.However, here we need more sophisticated tools.

```Use the final model to predict the PSA level for a patient whose quantitative predictors are at the sample means of the variables and qualitative predictors are at the most frequent category.```

```lm(formula = y ~ cancervol + vesinv + gleason + benpros)```
```{r}
table(gleason)
table(vesinv)
mean(cancervol)
mean(benpros)
```
From the above results we can see that gleason value 7 is being dominated in the data, vesinv value 0 is being dominated in the data and the mean of cancervol and benpros are 6.998 and 2.534 respectively. 

Predict the following values with the best linear model fit 2:
```{r}
summary(fit2)
```
predicted value is equal to:

``` -0.65013 + 6.998682*(0.06488) + 7*(0.33376) + 0.09136*(2.534725) = 2.371837  ```

```Thus, the actual value of PSA is exp(2.371837) which is equal to 10.71706```










